{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4: BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the official tokenization script created by the Google team\n",
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "#from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import math, os, re, time, random, string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "import tokenization\n",
    "from collections import defaultdict\n",
    "import wordcloud\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download BERT from the Tensorflow Hub\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)\n",
    "\n",
    "#Read CSV files \n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "#Download tokenizer from the bert layer\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "\n",
    "#- Encode the text into tokens, masks, and segment flags\n",
    "train_input = bert_encode(train.text.values, tokenizer, max_len=160)\n",
    "test_input = bert_encode(test.text.values, tokenizer, max_len=160)\n",
    "train_labels = train.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(x):\n",
    "    return x.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_stopwords(x):\n",
    "    return ' '.join([i for i in x.split() if i not in wordcloud.STOPWORDS])    \n",
    "\n",
    "def remove_non_alphabet(x):\n",
    "    return ' '.join([i for i in x.split() if i.isalpha()])\n",
    "\n",
    "#def strip_all_entities(x):\n",
    "#    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split())\n",
    "\n",
    "def remove_hashtag(x):\n",
    "    return \" \".join(word.strip() for word in re.split('#|_', x))\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x: x.lower())\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r'https?://\\S+|www\\.\\S+', '', x, flags = re.MULTILINE))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r'http?://\\S+|www\\.\\S+', '', x, flags = re.MULTILINE))\n",
    "train['text'] = train['text'].apply(remove_punctuation)\n",
    "train['text'] = train['text'].apply(remove_stopwords)\n",
    "train['text'] = train['text'].apply(remove_non_alphabet)\n",
    "train['text'] = train['text'].apply(remove_hashtag)\n",
    "train['text'] = train['text'].apply(lambda x : remove_URL(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST DATA\n",
    "test['text'] = test['text'].apply(lambda x: x.lower())\n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r'https?://\\S+|www\\.\\S+', '', x, flags = re.MULTILINE))\n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r'http?://\\S+|www\\.\\S+', '', x, flags = re.MULTILINE))\n",
    "test['text'] = test['text'].apply(remove_punctuation)\n",
    "test['text'] = test['text'].apply(remove_stopwords)\n",
    "test['text'] = test['text'].apply(remove_non_alphabet)\n",
    "train['text'] = train['text'].apply(remove_hashtag)\n",
    "train['text'] = train['text'].apply(lambda x : remove_URL(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Build, Train, Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(bert_layer, max_len=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/2\n",
      "6090/6090 [==============================] - 130s 21ms/sample - loss: 0.0673 - accuracy: 0.9732 - val_loss: 0.6100 - val_accuracy: 0.8424\n",
      "Epoch 2/2\n",
      "6090/6090 [==============================] - 127s 21ms/sample - loss: 0.0508 - accuracy: 0.9775 - val_loss: 0.7927 - val_accuracy: 0.8175\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n",
    "train_history = model.fit(train_input, train_labels, validation_split=0.2,\n",
    "    epochs=3, callbacks=[checkpoint], batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.h5')\n",
    "test_pred_BERT = pd.DataFrame()\n",
    "test_pred_BERT = model.predict(test_input)\n",
    "test_pred_BERT_int = test_pred_BERT.round().astype('int')\n",
    "train_pred_BERT = model.predict(train_input)\n",
    "train_pred_BERT_int = train_pred_BERT.round().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrecked stomach help\n",
      "0\n",
      "ohhmyjoshh stevenrulles gonna thinking gets shit wrecked first day school\n",
      "0\n",
      "wrecked tired gonna asleep\n",
      "0\n",
      "cramer igers words wrecked disneys stock\n",
      "0\n",
      "wrecked emotions\n",
      "0\n",
      "riddler best earlyexit primary presidential wannabe certain chances gets wrecked rich guy\n",
      "0\n",
      "marynmck thats beyond adorable hope wont wrecked now noticed\n",
      "0\n",
      "cramer igers words wrecked disneys stock cnbc topnews\n",
      "0\n",
      "caitsroberts see u night wee barra absolutely wrecked\n",
      "0\n",
      "kirafrog mountwario wrecked\n",
      "0\n",
      "awesome time gettin wrecked bowling last night\n",
      "1\n",
      "cramer words wrecked dis stock\n",
      "0\n",
      "bright side wrecked\n",
      "0\n",
      "wrecked\n",
      "0\n",
      "hes gone relax thought wife wrecked cake goner mind lol whoops\n",
      "0\n",
      "cameronhacker wrecked\n",
      "0\n",
      "three days work theyve pretty much wrecked hahaha shoutout family one\n",
      "0\n",
      "fx forex trading cramer igers words wrecked disneys stock\n",
      "1\n",
      "engineshed great atmosphere british lion gig tonight hearing wrecked\n",
      "1\n",
      "cramer igers words wrecked disneys stock cnbc\n",
      "1\n",
      "pic old pkk suicide bomber detonated bomb turkey army trench released\n",
      "1\n",
      "boxes ready explode exploding kittens finally arrived gameofkittens\n",
      "1\n",
      "calgary police flood road closures calgary\n",
      "1\n",
      "sismo detectado jap√¨n seismic intensity iwate miyagi jst\n",
      "1\n",
      "sirens everywhere\n",
      "1\n",
      "breaking isis claims responsibility mosque attack saudi arabia killed\n",
      "1\n",
      "omg earthquake\n",
      "1\n",
      "severe weather bulletin typhoon soudelor tropical cyclone warning issued pm\n",
      "1\n",
      "heat wave warning aa ayyo dei plan visit friends year\n",
      "0\n",
      "group suicide bomber detonated explosivespacked vest mosque inside saudi special forces headquarters killing people\n",
      "0\n",
      "heard really loud bang everyone asleep great\n",
      "0\n",
      "gas thing exploded heard screams now whole street smells gas\n",
      "0\n",
      "nws flash flood warning continued shelby county pm wednesday tnwx\n",
      "0\n",
      "rt livingsafely nws issues severe thunderstorm warnings parts ar nc ok expect trauma cases\n",
      "0\n",
      "aircraft debris found la reunion missing malaysia airlines\n",
      "0\n",
      "fatherofthree lost control car overtaking collided bathandnortheastsomerset\n",
      "0\n",
      "earthquake ssw anza california iphone users download earthquake app information\n",
      "0\n",
      "evacuation order lifted town roosevelt\n",
      "0\n",
      "breaking la refugio oil spill may costlier bigger projected\n",
      "0\n",
      "siren went wasnt forney tornado warning\n",
      "1\n",
      "officials say quarantine place alabama home possible ebola case developing symptoms\n",
      "1\n",
      "worldnews fallen powerlines glink tram update fire crews evacuated passengers tr\n",
      "1\n",
      "flip side im walmart bomb everyone evacuate stay tuned blow\n",
      "1\n",
      "suicide bomber kills saudi security site mosque reuters via world google news wall\n",
      "0\n",
      "stormchase violent record breaking el reno oklahoma tornado nearly runs\n",
      "1\n",
      "two giant cranes holding bridge collapse nearby homes\n",
      "1\n",
      "ariaahrary thetawniest control wild fires california even northern part state troubling\n",
      "1\n",
      "s volcano hawaii\n",
      "1\n",
      "police investigating ebike collided car little portugal ebike rider suffered serious nonlife threatening injuries\n",
      "1\n",
      "latest homes razed northern california wildfire abc news\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i, j in zip(train.text.values[-50:],submission_bert['target'][-50:]):\n",
    "    print(i)\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Showing Confusion Matrix\n",
    "def plot_cm(y_true, y_pred, title, figsize=(5,5)):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plt.title(title)\n",
    "    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n",
    "\n",
    "    # Showing Confusion Matrix for BERT model\n",
    "plot_cm(train_pred_BERT_int,train['target'].values,  'Confusion matrix for BERT model', figsize=(6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,make_scorer\n",
    "print('accuracy score: ',accuracy_score(train_pred_BERT_int,train[\"target\"].values))\n",
    "\n",
    "print(classification_report(train[\"target\"].values, train_pred_BERT_int))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
